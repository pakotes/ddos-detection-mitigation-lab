#!/usr/bin/env python3
"""
Script para verificar e validar os resultados dos modelos treinados
Analisa se 100% acurÃ¡cia Ã© realista ou indica overfitting
"""

import json
import numpy as np
from pathlib import Path
import joblib
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
import logging

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(message)s')
logger = logging.getLogger(__name__)

def validate_models():
    """Validar modelos de forma simples e clara"""
    
    models_dir = Path(__file__).parent.parent.parent / 'src' / 'models' / 'clean'
    datasets_dir = Path(__file__).parent.parent.parent / 'src' / 'datasets' / 'clean'
    
    if not models_dir.exists():
        logger.error("âŒ Modelos nÃ£o encontrados. Execute 'ddos-train-clean' primeiro.")
        return False
    
    print("ğŸ” VALIDAÃ‡ÃƒO DOS MODELOS - Verificando 100% AcurÃ¡cia")
    print("=" * 60)
    
    datasets = ['cicddos', 'unsw', 'integrated']
    all_perfect = True
    
    for dataset_name in datasets:
        print(f"\nğŸ“Š Dataset: {dataset_name.upper()}")
        print("-" * 30)
        
        try:
            # Carregar dados de teste
            X_test = np.load(datasets_dir / f'X_test_{dataset_name}.npy')
            y_test = np.load(datasets_dir / f'y_test_binary_{dataset_name}.npy')
            
            print(f"Amostras de teste: {len(X_test):,}")
            print(f"Ataques: {y_test.sum():,} ({y_test.mean():.1%})")
            print(f"Benignos: {(1-y_test).sum():,} ({(1-y_test.mean()):.1%})")
            
            # Verificar Random Forest (principal modelo)
            rf_model = joblib.load(models_dir / f'random_forest_{dataset_name}.pkl')
            y_pred = rf_model.predict(X_test)
            y_proba = rf_model.predict_proba(X_test)
            
            # Calcular mÃ©tricas
            accuracy = (y_pred == y_test).mean()
            errors = (y_pred != y_test).sum()
            confidence = np.max(y_proba, axis=1)
            
            print(f"âœ“ AcurÃ¡cia: {accuracy:.4f} ({accuracy*100:.1f}%)")
            print(f"âœ“ Erros: {errors}/{len(y_test)}")
            print(f"âœ“ ConfianÃ§a mÃ©dia: {confidence.mean():.3f}")
            
            # Matriz de confusÃ£o
            cm = confusion_matrix(y_test, y_pred)
            print(f"âœ“ Matriz confusÃ£o: TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}")
            
            # Verificar se Ã© overfitting
            if errors == 0:
                print("âš ï¸  ZERO ERROS - PossÃ­vel overfitting!")
                all_perfect = False
            
            if confidence.min() > 0.95:
                print("âš ï¸  ConfianÃ§a muito alta - Dados podem ser muito simples")
                
        except Exception as e:
            print(f"âŒ Erro: {e}")
            continue
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ ANÃLISE GERAL:")
    
    if all_perfect:
        print("ğŸš¨ TODOS OS MODELOS TÃŠM 100% ACURÃCIA")
        print("ğŸ“Œ Isso pode indicar:")
        print("   â€¢ Dados sintÃ©ticos muito bem separados")
        print("   â€¢ PadrÃµes Ã³bvios entre ataques e trÃ¡fego normal")
        print("   â€¢ PossÃ­vel overfitting")
        print("   â€¢ Para dados reais, espere 85-95% acurÃ¡cia")
    else:
        print("âœ… Modelos parecem realistas")
    
    print("\nğŸ’¡ RECOMENDAÃ‡Ã•ES:")
    print("   â€¢ Para testes: 100% estÃ¡ OK (dados sintÃ©ticos)")
    print("   â€¢ Para produÃ§Ã£o: Use dados reais do CIC-DDoS2019")
    print("   â€¢ Considere adicionar ruÃ­do aos dados sintÃ©ticos")
    print("   â€¢ Teste com dados nÃ£o vistos durante treino")
    
    return True

if __name__ == "__main__":
    validate_models()
